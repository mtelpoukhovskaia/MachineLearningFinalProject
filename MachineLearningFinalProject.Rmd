---
title: "Predicting Weight Lifting Motion"
author: "Maria Telpoukhovskaia"
date: "January 24, 2017"
output: html_document
keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
knitr::opts_chunk$set(cache=TRUE, warning=FALSE, message=FALSE)
library(ggplot2); library(caret)
```


## Synopsis

This report presents a prediction model for metrics while people are performing different kinds of weight lifting exercises. 

The data is found in csv format, training set and testing set: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data set has parameters from accelerometers on 6 people's belt, forearm, arm, and dumbbell while they are doing barbell lifts correctly and incorrectly.

Using this data from accelerometers, we will build a model that predicts how the action is performed - correctly or incorrectly. We will use cross validation and out of sample error rate to assess how well the models work. From this, the best model is chosen to predict 20 test cases.  All the analysis is done with R, and several packages, in particular, the caret package.

## Data Processing

First, we load the training and the testing sets, ensuring that all data points that are not entered are marked as NA.  We check the dimensions of the two sets.

```{r}
df <- read.csv("pml-training.csv", na.strings=c(""," ","NA"))
testing.final.20 <- read.csv("pml-testing.csv", na.strings=c(""," ","NA"))

dim(df); dim(testing.final.20)
```


We look at the summary of the training data set, and see that many columns have a large number of missing values.  These are removed, creating a data set called training.data.

```{r}
head(df[,c(1:20)])
cols.with.data <- names(which(colSums(is.na(df))==0))
df.data <- subset(df, select=cols.with.data)
```

As well, we see that the first 11 columns are not readings from the accellerometer, so we remove them from the training data set.

```{r}
df.data <- df.data[,-c(1:11)]
dim(df.data)
```

Now, we are going to split the data into training, testing, and validation sets.

```{r}
inBuild <- createDataPartition(y=df.data$classe,
                               p=0.7, list=FALSE)
validation <- df.data[-inBuild,]; buildData <- df.data[inBuild,]

inTrain <- createDataPartition(y=buildData$classe,
                               p=0.7, list=FALSE)
training <- buildData[inTrain,]; testing <- buildData[-inTrain,]

dim(training)
dim(testing)
dim(validation)
```


Now we are going to examine data.  We can't represent all the data for this report, but let's examine first three with a feature plot to look at pair wise relationships. 


```{r}
featurePlot(x=training[,c(1:3)],
            y = training$classe,
            plot="pairs")
```

There may be some relationship between the variables, so we will look at the ones that have 90% covariance to take them out of the models.


```{r}
M <- abs(cor(training[,-49])) ##49 is the outcome
diag(M) <- 0 ## takes out diagonal
which(M > 0.9,arr.ind=T)

training <- training[,-c(6, 15, 29, 42)]
```


We treat the testing and the validation sets the same, without looking at them.

```{r}
testing <- testing[,-c(6, 15, 29, 42)]
validation <- validation[,-c(6, 15, 29, 42)]
```


##Building models on the testing set

There are 5 different outcomes (classe), and we will use three models to predict them: lda, rf, and gbm. We can't use models that only predict two outcomes. We will also build models by pre-processing the data with PCA first.

```{r}
set.seed(1)

mod1 <- train(classe ~.,method="lda",data=training)
mod2 <- train(classe ~.,method="rf",data=training)
mod3 <- train(classe ~.,method="gbm",data=training, verbose=FALSE)

mod1.pca <- train(classe ~.,method="lda",preProcess="pca",data=training)
mod2.pca <- train(classe ~.,method="rf",preProcess="pca",data=training)
mod3.pca <- train(classe ~.,method="gbm",preProcess="pca",data=training, verbose=FALSE)
```

We make predictions on the testing set, and combine them into a new data set for a combination rf prediction.

```{r}
pred1 <- predict(mod1,testing); pred2 <- predict(mod2,testing); pred3 <- predict(mod3,testing)

pred1.pca <- predict(mod1.pca,testing); pred2.pca <- predict(mod2.pca,testing); pred3.pca <- predict(mod3.pca,testing)

predDF <- data.frame(pred1,pred2,pred3, classe=testing$classe)
combModFit <- train(classe ~.,method="rf",data=predDF)
combPred <- predict(combModFit,predDF)

predDF.pca <- data.frame(pred1.pca,pred2.pca,pred3.pca, classe=testing$classe)
combModFit.pca <- train(classe ~.,method="rf",data=predDF.pca)
combPred.pca <- predict(combModFit.pca,predDF.pca)
```

We check for accuracy of prediction on the testing set. Results are displayed as % accuracy after each line.

```{r, eval=FALSE}
confusionMatrix(testing$classe,pred1) ##66%
confusionMatrix(testing$classe,pred2) ##98%
confusionMatrix(testing$classe,pred3) ##92%
confusionMatrix(testing$classe,combPred) ##98%

confusionMatrix(testing$classe,pred1.pca) ##52%
confusionMatrix(testing$classe,pred2.pca) ##96%
confusionMatrix(testing$classe,pred3.pca) ##79%
confusionMatrix(testing$classe,combPred.pca) ##96%
```

Random forest (rf) is the best single model to use, with accuracy of 98%.  The combination model produces a somewhat better accuracy, but improvement is minimal, less than 0.2%.

From this, we select the random forest prediction model, without PCA preproccessing.

We predict on the validation set to determine the out of sample error.

```{r}
finalPred <- predict(mod2,validation)
confusionMatrix(validation$classe,finalPred)
```

Accuracy is 98%.

Finally, let's predict the 20 samples that were in another data set - they don't have the classe column.

```{r}
cols.with.data <- names(which(colSums(is.na(testing.final.20))==0))
t.f.20.data <- subset(testing.final.20, select=cols.with.data)

t.f.20.data <- t.f.20.data[,-c(1:11)]
t.f.20.data <- t.f.20.data[,-c(6, 15, 29, 42)]

t.f20.Pred <- predict(mod2,t.f.20.data)
t.f20.Pred
```

Predictions are: B A B A A E D B A A B C B A E E A B B B

##Conclusions

We predicted how 20 test cases performed the exercise with machine learning prediction modeling.